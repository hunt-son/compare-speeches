{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'Raw_debates')\n",
    "RESULT_DIR = os.path.join(ROOT_DIR, 'candidate_lines')\n",
    "\n",
    "candidates = {'CLINTON', 'SANDERS', 'TRUMP', 'RUBIO', 'CRUZ'}\n",
    "cand_list = sorted(candidates)\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "numWords = 1000\n",
    "minLength = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dic = {'action': 'act',\n",
    "               'agreement': 'agree',\n",
    "               'americans': 'american',\n",
    "               'asked': 'ask',\n",
    "               'asking': 'ask',\n",
    "               'going': 'go',\n",
    "               'states': 'state',\n",
    "               'working': 'work',\n",
    "               'millions': 'million',\n",
    "               'bringing': 'bring',\n",
    "               'businesses': 'business',\n",
    "               'candidates': 'candidate',\n",
    "               'children': 'child',\n",
    "               'comes': 'come',\n",
    "               'coming': 'come',\n",
    "               'companies': 'company',\n",
    "               'countries': 'country',\n",
    "               'deals': 'deal',\n",
    "               'economic': 'economy',\n",
    "               'families': 'family',\n",
    "               'fighting': 'fight',\n",
    "               'gets': 'get',\n",
    "               'getting': 'get',\n",
    "               'goes': 'go',\n",
    "               'got': 'get',\n",
    "               'groups': 'group',\n",
    "               'guns': 'gun',\n",
    "               'happened': 'happen',\n",
    "               'happening': 'happen',\n",
    "               'helped': 'help',\n",
    "               'issues': 'issue',\n",
    "               'knows': 'know',\n",
    "               'laws': 'law',\n",
    "               'lives': 'live',\n",
    "               'living': 'live',\n",
    "               'making': 'make',\n",
    "               'needs': 'need',\n",
    "               'passed': 'pass',\n",
    "               'problems': 'problem',\n",
    "               'putting': 'put',\n",
    "               'really': 'real',\n",
    "               'republicans': 'republican',\n",
    "               'running': 'run',\n",
    "               'saying': 'say',\n",
    "               'said': 'say',\n",
    "               'seeing': 'see',\n",
    "               'seen': 'see',\n",
    "               'started': 'start',\n",
    "               'supported': 'support',\n",
    "               'taking': 'take',\n",
    "               'talked': 'talk',\n",
    "               'talking': 'talk',\n",
    "               'terrorists': 'terrorist',\n",
    "               'terrorism': 'terrorist',\n",
    "               'things': 'thing',\n",
    "               'trying': 'try',\n",
    "               'used': 'use',\n",
    "               'using': 'use',\n",
    "               'voted': 'vote',\n",
    "               'wages': 'wage',\n",
    "               'wanted': 'want',\n",
    "               'wants': 'want',\n",
    "               'building': 'build',\n",
    "               'called': 'call',\n",
    "               'came': 'come',\n",
    "               'communities': 'community',\n",
    "               'costs': 'cost',\n",
    "               'deffence': 'deffend',\n",
    "               'difference': 'different',\n",
    "               'drugs': 'drug',\n",
    "               'gave': 'give',\n",
    "               'given': 'give',\n",
    "               'gone': 'go',\n",
    "               'higher': 'high',\n",
    "               'highest': 'high',\n",
    "               'interests': 'interest',\n",
    "               'jobs': 'job',\n",
    "               'longer': 'long',\n",
    "               'looked': 'look',\n",
    "               'looking': 'look',\n",
    "               'lost': 'lose',\n",
    "               'made': 'make',\n",
    "               'means': 'mean',\n",
    "               'paying': 'pay',\n",
    "               'planned': 'plan',\n",
    "               'programs': 'program',\n",
    "               'raising': 'raise',\n",
    "               'reasons': 'reason',\n",
    "               'ringing': 'ring',\n",
    "               'says': 'say',\n",
    "               'saw': 'see',\n",
    "               'wealthy': 'wealth',\n",
    "               'worked': 'work',\n",
    "               'years': 'year'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_raw_text(cand_name):\n",
    "    file_name = '{0}.txt'.format(cand_name)\n",
    "    file_path = os.path.join(RESULT_DIR, file_name)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return f.read()\n",
    "\n",
    "def raw_to_blocks(cand_name, raw_text, delimiter='\\r\\n\\r\\n', min_len=minLength):\n",
    "    # Delete candidate's name\n",
    "    names_removed = raw_text.replace(cand_name + ':', '')\n",
    "    \n",
    "    # Split the text into paragraphs\n",
    "    splitted = names_removed.split(delimiter)\n",
    "\n",
    "    # Gather only long enough paragraphs\n",
    "    rets = [paragraph for paragraph in splitted if len(paragraph) > min_len]\n",
    "    return rets\n",
    "\n",
    "# Replace words that are virtually same into one word. \n",
    "def hard_code_process(text):\n",
    "    separate_word = ' {0} '\n",
    "    text = separate_word.format(text)\n",
    "    \n",
    "    for key, value in word_dic.iteritems():\n",
    "        key = separate_word.format(key)\n",
    "        value = separate_word.format(value)\n",
    "        text = text.replace(key, value)\n",
    "    return text\n",
    "\n",
    "def process_paragraph(paragraph):\n",
    "    # Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", paragraph) \n",
    "    \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    rets = ' '.join(meaningful_words)\n",
    "    return hard_code_process(rets)\n",
    "\n",
    "def get_processed_lines(cand_name, delimiter='\\r\\n\\r\\n', min_len=minLength):\n",
    "    raw_text = get_raw_text(cand_name)\n",
    "    blocks = raw_to_blocks(cand_name, raw_text, \n",
    "                           delimiter=delimiter, min_len=min_len)\n",
    "    return map(process_paragraph, blocks)\n",
    "\n",
    "def get_vectorizer(cand_lines, max_features=1000):\n",
    "    # Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "    # bag of words tool.  \n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 max_features = max_features)\n",
    "    \n",
    "    concatenated = []\n",
    "    for cand_name, lines in cand_lines.iteritems():\n",
    "        concatenated += lines\n",
    "    \n",
    "    vectorizer.fit(concatenated)\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Examples\n",
    "cand_lines = {c: get_processed_lines(c) for c in candidates}\n",
    "vectorizer = get_vectorizer(cand_lines, numWords)\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "clinton_train = vectorizer.transform(cand_lines['CLINTON']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split training data and test data\n",
    "def test_train_split(cand_lines, test_ratio=0.25):\n",
    "    cands = cand_lines.keys()\n",
    "    test_data = dict.fromkeys(cands)\n",
    "    train_data = dict.fromkeys(cands)\n",
    "    for c in cands:\n",
    "        lines = cand_lines[c]\n",
    "        l = len(lines)\n",
    "        test_len = int(l * test_ratio)\n",
    "        test_indices = set(np.random.choice(l, test_len, replace=False))\n",
    "        train_indices = set(range(l)) - test_indices\n",
    "        test_data[c] = list(np.take(lines, list(test_indices)))\n",
    "        train_data[c] = list(np.take(lines, list(train_indices)))\n",
    "    return test_data, train_data\n",
    "\n",
    "def normalize(data):\n",
    "    rets = dict()\n",
    "    for c in data:\n",
    "        d = data[c].astype(float)\n",
    "        _sum = d.sum(axis=1)\n",
    "        _sum[_sum==0] = 1\n",
    "#         rets[c] = (d.T).T\n",
    "        rets[c] = (d.T / _sum).T\n",
    "    return rets\n",
    "\n",
    "def normalizeBinary(data):\n",
    "    rets = dict()\n",
    "    for c in data:\n",
    "        d = data[c].astype(float)\n",
    "        d[d!=0] = 1\n",
    "       \n",
    "        rets[c] = d\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process the data\n",
    "\n",
    "cand_lines = {c: get_processed_lines(c) for c in candidates}\n",
    "test_data, train_data = test_train_split(cand_lines, 0.01)\n",
    "\n",
    "vectorizer = get_vectorizer(train_data, numWords)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "for c in train_data:\n",
    "    train_data[c] = vectorizer.transform(train_data[c]).toarray()\n",
    "    test_data[c] = vectorizer.transform(test_data[c]).toarray()\n",
    "    \n",
    "# Processing proportional, unnormalized data\n",
    "\n",
    "train_x_reg, test_x_reg = None, None\n",
    "train_y_reg, test_y_reg = [], []\n",
    "\n",
    "for c in train_data:\n",
    "    i  = cand_list.index(c)\n",
    "    if train_x_reg is None:\n",
    "        train_x_reg = train_data[c]\n",
    "    else:\n",
    "        train_x_reg = np.concatenate((train_x_reg, train_data[c]))\n",
    "    train_y_reg += [i] * len(train_data[c])\n",
    "    \n",
    "for c in test_data:\n",
    "    i  = cand_list.index(c)\n",
    "    if test_x_reg is None:\n",
    "        test_x_reg = test_data[c]\n",
    "    else:\n",
    "        test_x_reg = np.concatenate((test_x_reg, test_data[c]))\n",
    "    test_y_reg += [i] * len(test_data[c])\n",
    "    \n",
    "# Processing proportional data\n",
    "\n",
    "train_data = normalize(train_data)\n",
    "test_data = normalize(test_data)\n",
    "\n",
    "train_x, test_x = None, None\n",
    "train_y, test_y = [], []\n",
    "\n",
    "for c in train_data:\n",
    "    i  = cand_list.index(c)\n",
    "    if train_x is None:\n",
    "        train_x = train_data[c]\n",
    "    else:\n",
    "        train_x = np.concatenate((train_x, train_data[c]))\n",
    "    train_y += [i] * len(train_data[c])\n",
    "    \n",
    "for c in test_data:\n",
    "    i  = cand_list.index(c)\n",
    "    if test_x is None:\n",
    "        test_x = test_data[c]\n",
    "    else:\n",
    "        test_x = np.concatenate((test_x, test_data[c]))\n",
    "    test_y += [i] * len(test_data[c])\n",
    "    \n",
    "# Processing binary data\n",
    "\n",
    "train_dataBin = normalizeBinary(train_data)\n",
    "test_dataBin = normalizeBinary(test_data)\n",
    "\n",
    "train_xBin, test_xBin = None, None\n",
    "train_yBin, test_yBin = [], []\n",
    "\n",
    "for c in train_dataBin:\n",
    "    i  = cand_list.index(c)\n",
    "    if train_xBin is None:\n",
    "        train_xBin = train_dataBin[c]\n",
    "    else:\n",
    "        train_xBin = np.concatenate((train_xBin, train_dataBin[c]))\n",
    "    train_yBin += [i] * len(train_dataBin[c])\n",
    "    \n",
    "for c in test_dataBin:\n",
    "    i  = cand_list.index(c)\n",
    "    if test_xBin is None:\n",
    "        test_xBin = test_dataBin[c]\n",
    "    else:\n",
    "        test_xBin = np.concatenate((test_xBin, test_dataBin[c]))\n",
    "    test_yBin += [i] * len(test_dataBin[c])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.0, 5.0, 5.0, 5.0, 5.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 24.0, 24.0, 24.0, 24.0, 24.0, 24.0, 24.0, 24.0, 24.0, 24.0, 24.0, 24.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 26.0, 26.0, 26.0, 26.0, 26.0, 26.0, 26.0, 26.0, 26.0, 26.0, 26.0, 26.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 29.0, 29.0, 29.0, 29.0, 29.0, 29.0, 29.0, 29.0, 29.0, 29.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 33.0, 33.0, 33.0, 33.0, 33.0, 33.0, 33.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 35.0, 35.0, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 38.0, 38.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 43.0, 43.0, 43.0, 43.0, 43.0, 44.0, 44.0, 44.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 46.0, 46.0, 46.0, 46.0, 47.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 49.0, 49.0, 49.0, 50.0, 50.0, 50.0, 50.0, 50.0, 51.0, 51.0, 51.0, 51.0, 51.0, 52.0, 52.0, 52.0, 53.0, 53.0, 53.0, 54.0, 54.0, 55.0, 55.0, 55.0, 56.0, 56.0, 56.0, 56.0, 56.0, 56.0, 57.0, 57.0, 57.0, 57.0, 57.0, 58.0, 58.0, 58.0, 58.0, 59.0, 59.0, 59.0, 60.0, 60.0, 61.0, 61.0, 62.0, 62.0, 62.0, 62.0, 63.0, 63.0, 63.0, 63.0, 63.0, 64.0, 64.0, 64.0, 65.0, 66.0, 67.0, 67.0, 68.0, 68.0, 68.0, 68.0, 69.0, 69.0, 69.0, 70.0, 70.0, 71.0, 71.0, 71.0, 72.0, 72.0, 73.0, 73.0, 74.0, 74.0, 75.0, 76.0, 76.0, 76.0, 76.0, 77.0, 77.0, 77.0, 78.0, 79.0, 79.0, 80.0, 81.0, 82.0, 82.0, 82.0, 83.0, 83.0, 86.0, 86.0, 88.0, 88.0, 88.0, 90.0, 91.0, 91.0, 92.0, 92.0, 93.0, 94.0, 94.0, 94.0, 94.0, 94.0, 96.0, 97.0, 97.0, 98.0, 100.0, 101.0, 103.0, 104.0, 104.0, 105.0, 106.0, 106.0, 107.0, 107.0, 107.0, 109.0, 111.0, 112.0, 112.0, 113.0, 115.0, 117.0, 117.0, 117.0, 121.0, 121.0, 123.0, 123.0, 124.0, 126.0, 126.0, 129.0, 132.0, 133.0, 135.0, 142.0, 145.0, 147.0, 147.0, 147.0, 150.0, 153.0, 156.0, 157.0, 159.0, 163.0, 165.0, 168.0, 169.0, 171.0, 177.0, 183.0, 191.0, 193.0, 195.0, 206.0, 207.0, 208.0, 214.0, 222.0, 227.0, 228.0, 234.0, 252.0, 276.0, 278.0, 293.0, 296.0, 297.0, 309.0, 318.0, 343.0, 366.0, 379.0, 421.0, 454.0, 491.0, 507.0, 587.0, 615.0]\n"
     ]
    }
   ],
   "source": [
    "counts = []\n",
    "for i in xrange(len(train_x[0,:])):\n",
    "    counts.append(sum(train_xBin[:,i]) + sum(test_xBin[:,i]))\n",
    "\n",
    "print(sorted(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nTree: 100, depth: 50\n",
      "1.25675296783\n",
      "1.13967013359\n",
      "nTree: 100, depth: 100\n",
      "1.53021001816\n",
      "1.44566106796\n",
      "nTree: 100, depth: 500\n",
      "1.72514295578\n",
      "1.22407102585\n",
      "nTree: 100, depth: 1000\n",
      "1.28824400902\n",
      "1.20889997482\n",
      "nTree: 500, depth: 50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e91cbedefabb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtempRF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtempRF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mfinish\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 273\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \"\"\"\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_counts\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    302\u001b[0m                                            max_leaf_nodes)\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_nTree = []\n",
    "_depth = []\n",
    "_train = []\n",
    "_test = []\n",
    "_timeTaken = []\n",
    "\n",
    "_trainBin = []\n",
    "_testBin = []\n",
    "_timeTakenBin = []\n",
    "\n",
    "tryNTree = [100,500,2000]\n",
    "tryDepth = [50,100,500,1000]\n",
    "\n",
    "for nTree in tryNTree:\n",
    "    for depth in tryDepth:\n",
    "        \n",
    "        print('nTree: {0}, depth: {1}'.format(nTree,depth))\n",
    "        \n",
    "        start = time.time()\n",
    "        tempRF = RandomForestClassifier(n_estimators=nTree, max_depth=depth)\n",
    "        tempRF.fit(train_x, train_y)\n",
    "        finish = time.time() - start\n",
    "        print(finish)\n",
    "        \n",
    "        _timeTaken.append(finish)\n",
    "        _nTree.append(nTree)\n",
    "        _depth.append(depth)\n",
    "        _train.append(tempRF.score(train_x, train_y))\n",
    "        _test.append(tempRF.score(test_x, test_y))\n",
    "        \n",
    "        start = time.time()\n",
    "        tempRF = RandomForestClassifier(n_estimators=nTree, max_depth=depth)\n",
    "        tempRF.fit(train_xBin, train_yBin)\n",
    "        finish = time.time() - start\n",
    "        print(finish)\n",
    "        \n",
    "        _timeTakenBin.append(finish)\n",
    "        _trainBin.append(tempRF.score(train_xBin, train_yBin))\n",
    "        _testBin.append(tempRF.score(test_xBin, test_yBin))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depth</th>\n",
       "      <th>nTree</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_scoreBin</th>\n",
       "      <th>time taken</th>\n",
       "      <th>train_score</th>\n",
       "      <th>train_scoreBin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>0.687379</td>\n",
       "      <td>0.708738</td>\n",
       "      <td>1.256753</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.664078</td>\n",
       "      <td>0.708738</td>\n",
       "      <td>1.530210</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>100</td>\n",
       "      <td>0.646602</td>\n",
       "      <td>0.689320</td>\n",
       "      <td>1.725143</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.666019</td>\n",
       "      <td>0.700971</td>\n",
       "      <td>1.288244</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   depth  nTree  test_score  test_scoreBin  time taken  train_score  \\\n",
       "0     50    100    0.687379       0.708738    1.256753            1   \n",
       "1    100    100    0.664078       0.708738    1.530210            1   \n",
       "2    500    100    0.646602       0.689320    1.725143            1   \n",
       "3   1000    100    0.666019       0.700971    1.288244            1   \n",
       "\n",
       "   train_scoreBin  \n",
       "0               1  \n",
       "1               1  \n",
       "2               1  \n",
       "3               1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame({'nTree': _nTree, 'depth': _depth, 'train_score': _train, 'test_score': _test,'train_scoreBin': _trainBin, 'test_scoreBin': _testBin, 'time taken': _timeTaken})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[70,  5,  8, 15, 11],\n",
       "       [ 4, 65, 11,  2, 16],\n",
       "       [ 7, 12, 77,  3,  9],\n",
       "       [ 8,  1, 13, 58, 18],\n",
       "       [ 4,  5,  3,  4, 86]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y,tempRF.predict(test_xBin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4]), array([109,  98, 108,  98, 102]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result = tempRF.predict_proba(np.eye(1500))\n",
    "np.unique(test_y, return_counts = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.2895</td>\n",
       "      <td>0.5735</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.2975</td>\n",
       "      <td>0.5405</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.1005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0275</td>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.2835</td>\n",
       "      <td>0.5525</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.2980</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0535</td>\n",
       "      <td>0.2805</td>\n",
       "      <td>0.5315</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.1115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.2940</td>\n",
       "      <td>0.5575</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.1090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.2775</td>\n",
       "      <td>0.5465</td>\n",
       "      <td>0.0575</td>\n",
       "      <td>0.0945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>0.5565</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.2945</td>\n",
       "      <td>0.5675</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.0975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.2905</td>\n",
       "      <td>0.5695</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.0965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.3340</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0695</td>\n",
       "      <td>0.2830</td>\n",
       "      <td>0.5045</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.1030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.2920</td>\n",
       "      <td>0.5700</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.0970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0365</td>\n",
       "      <td>0.2820</td>\n",
       "      <td>0.5115</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.1470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0505</td>\n",
       "      <td>0.2700</td>\n",
       "      <td>0.5520</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.2920</td>\n",
       "      <td>0.5675</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.5620</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0175</td>\n",
       "      <td>0.3190</td>\n",
       "      <td>0.5475</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.3040</td>\n",
       "      <td>0.5335</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.2880</td>\n",
       "      <td>0.5580</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.0950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.2915</td>\n",
       "      <td>0.5505</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.1165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.2810</td>\n",
       "      <td>0.5840</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0320</td>\n",
       "      <td>0.2890</td>\n",
       "      <td>0.5610</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.2805</td>\n",
       "      <td>0.5845</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.0950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.2760</td>\n",
       "      <td>0.2355</td>\n",
       "      <td>0.3990</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.0680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0290</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.0325</td>\n",
       "      <td>0.0920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.2600</td>\n",
       "      <td>0.5025</td>\n",
       "      <td>0.1290</td>\n",
       "      <td>0.0900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.2905</td>\n",
       "      <td>0.5720</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0715</td>\n",
       "      <td>0.2680</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.0940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.2920</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.1030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>0.0265</td>\n",
       "      <td>0.2880</td>\n",
       "      <td>0.5575</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.1070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>0.0470</td>\n",
       "      <td>0.2765</td>\n",
       "      <td>0.5640</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473</th>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.2735</td>\n",
       "      <td>0.5595</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.1070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.3015</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475</th>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.2920</td>\n",
       "      <td>0.5745</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.0955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476</th>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.3525</td>\n",
       "      <td>0.4810</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.1095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477</th>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>0.5615</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.1030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>0.0170</td>\n",
       "      <td>0.3335</td>\n",
       "      <td>0.5355</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>0.1295</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.4600</td>\n",
       "      <td>0.1330</td>\n",
       "      <td>0.0675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.2920</td>\n",
       "      <td>0.5635</td>\n",
       "      <td>0.0275</td>\n",
       "      <td>0.0975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.3015</td>\n",
       "      <td>0.5420</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.1130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>0.0750</td>\n",
       "      <td>0.2725</td>\n",
       "      <td>0.5210</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>0.0515</td>\n",
       "      <td>0.1735</td>\n",
       "      <td>0.6220</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.2835</td>\n",
       "      <td>0.5620</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>0.0955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.2855</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.2760</td>\n",
       "      <td>0.5365</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.1470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>0.0460</td>\n",
       "      <td>0.3995</td>\n",
       "      <td>0.4525</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.0795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>0.0175</td>\n",
       "      <td>0.2945</td>\n",
       "      <td>0.5595</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>0.0990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.2890</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>0.0515</td>\n",
       "      <td>0.0970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.2785</td>\n",
       "      <td>0.5820</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>0.0945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.2845</td>\n",
       "      <td>0.5890</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>0.0580</td>\n",
       "      <td>0.2760</td>\n",
       "      <td>0.5410</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.0875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.2990</td>\n",
       "      <td>0.5585</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.2495</td>\n",
       "      <td>0.4780</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>0.2245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>0.1490</td>\n",
       "      <td>0.2360</td>\n",
       "      <td>0.4290</td>\n",
       "      <td>0.0975</td>\n",
       "      <td>0.0885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.2895</td>\n",
       "      <td>0.5615</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>0.0970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>0.5610</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>0.0955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.2915</td>\n",
       "      <td>0.5685</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.0985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0       1       2       3       4\n",
       "0     0.0185  0.2895  0.5735  0.0210  0.0975\n",
       "1     0.0410  0.2975  0.5405  0.0205  0.1005\n",
       "2     0.0275  0.2590  0.5730  0.0345  0.1060\n",
       "3     0.0180  0.2835  0.5525  0.0195  0.1265\n",
       "4     0.0190  0.2980  0.5650  0.0205  0.0975\n",
       "5     0.0535  0.2805  0.5315  0.0230  0.1115\n",
       "6     0.0190  0.2940  0.5575  0.0205  0.1090\n",
       "7     0.0240  0.2775  0.5465  0.0575  0.0945\n",
       "8     0.0190  0.2875  0.5565  0.0395  0.0975\n",
       "9     0.0190  0.2945  0.5675  0.0215  0.0975\n",
       "10    0.0190  0.2905  0.5695  0.0245  0.0965\n",
       "11    0.0215  0.3340  0.5255  0.0200  0.0990\n",
       "12    0.0695  0.2830  0.5045  0.0400  0.1030\n",
       "13    0.0190  0.2920  0.5700  0.0220  0.0970\n",
       "14    0.0365  0.2820  0.5115  0.0230  0.1470\n",
       "15    0.0505  0.2700  0.5520  0.0370  0.0905\n",
       "16    0.0215  0.2920  0.5675  0.0210  0.0980\n",
       "17    0.0250  0.2800  0.5620  0.0400  0.0930\n",
       "18    0.0175  0.3190  0.5475  0.0200  0.0960\n",
       "19    0.0530  0.3040  0.5335  0.0180  0.0915\n",
       "20    0.0375  0.2880  0.5580  0.0215  0.0950\n",
       "21    0.0205  0.2915  0.5505  0.0210  0.1165\n",
       "22    0.0195  0.2810  0.5840  0.0205  0.0950\n",
       "23    0.0320  0.2890  0.5610  0.0210  0.0970\n",
       "24    0.0185  0.2805  0.5845  0.0215  0.0950\n",
       "25    0.2760  0.2355  0.3990  0.0215  0.0680\n",
       "26    0.0290  0.2565  0.5900  0.0325  0.0920\n",
       "27    0.0185  0.2600  0.5025  0.1290  0.0900\n",
       "28    0.0200  0.2905  0.5720  0.0210  0.0965\n",
       "29    0.0715  0.2680  0.5255  0.0410  0.0940\n",
       "...      ...     ...     ...     ...     ...\n",
       "1470  0.0190  0.2920  0.5650  0.0210  0.1030\n",
       "1471  0.0265  0.2880  0.5575  0.0210  0.1070\n",
       "1472  0.0470  0.2765  0.5640  0.0205  0.0920\n",
       "1473  0.0205  0.2735  0.5595  0.0395  0.1070\n",
       "1474  0.0185  0.3015  0.5625  0.0205  0.0970\n",
       "1475  0.0185  0.2920  0.5745  0.0195  0.0955\n",
       "1476  0.0375  0.3525  0.4810  0.0195  0.1095\n",
       "1477  0.0190  0.2960  0.5615  0.0205  0.1030\n",
       "1478  0.0180  0.2960  0.5380  0.0200  0.1280\n",
       "1479  0.0170  0.3335  0.5355  0.0205  0.0935\n",
       "1480  0.1295  0.2100  0.4600  0.1330  0.0675\n",
       "1481  0.0195  0.2920  0.5635  0.0275  0.0975\n",
       "1482  0.0185  0.3015  0.5420  0.0250  0.1130\n",
       "1483  0.0750  0.2725  0.5210  0.0370  0.0945\n",
       "1484  0.0515  0.1735  0.6220  0.0270  0.1260\n",
       "1485  0.0230  0.2835  0.5620  0.0360  0.0955\n",
       "1486  0.0190  0.2855  0.5780  0.0200  0.0975\n",
       "1487  0.0160  0.2760  0.5365  0.0245  0.1470\n",
       "1488  0.0460  0.3995  0.4525  0.0225  0.0795\n",
       "1489  0.0175  0.2945  0.5595  0.0295  0.0990\n",
       "1490  0.0235  0.2890  0.5390  0.0515  0.0970\n",
       "1491  0.0195  0.2785  0.5820  0.0255  0.0945\n",
       "1492  0.0135  0.2845  0.5890  0.0190  0.0940\n",
       "1493  0.0580  0.2760  0.5410  0.0375  0.0875\n",
       "1494  0.0195  0.2990  0.5585  0.0260  0.0970\n",
       "1495  0.0225  0.2495  0.4780  0.0255  0.2245\n",
       "1496  0.1490  0.2360  0.4290  0.0975  0.0885\n",
       "1497  0.0225  0.2895  0.5615  0.0295  0.0970\n",
       "1498  0.0195  0.2885  0.5610  0.0355  0.0955\n",
       "1499  0.0195  0.2915  0.5685  0.0220  0.0985\n",
       "\n",
       "[1500 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-9a78749b582e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "result = Series(result)\n",
    "result.describe()\n",
    "\n",
    "for i in range(5):\n",
    "    print i, (result==i).sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(confusion_matrix(test_y,tempRF.predict(test_xBin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CLINTON', 'CRUZ', 'RUBIO', 'SANDERS', 'TRUMP']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optimize C and gamma\n",
    "\n",
    "_C = []\n",
    "_gamma = []\n",
    "_train = []\n",
    "_test = []\n",
    "\n",
    "for C in [0.01, 0.1, 1, 10, 100]:\n",
    "    for gamma in [0.01, 0.1, 1, 10, 100]:\n",
    "        clf = svm.SVC(C=C, gamma=gamma)\n",
    "        clf.fit(train_x, train_y)\n",
    "        _C.append(C)\n",
    "        _gamma.append(gamma)\n",
    "        _train.append(clf.score(train_x, train_y))\n",
    "        _test.append(clf.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>gamma</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.211650</td>\n",
       "      <td>0.210560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.211650</td>\n",
       "      <td>0.210560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.211650</td>\n",
       "      <td>0.210560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.211650</td>\n",
       "      <td>0.210560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.211650</td>\n",
       "      <td>0.210560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.211650</td>\n",
       "      <td>0.210560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.211650</td>\n",
       "      <td>0.210560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.211650</td>\n",
       "      <td>0.210560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.10</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.322330</td>\n",
       "      <td>0.364456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.10</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.211650</td>\n",
       "      <td>0.210560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.211650</td>\n",
       "      <td>0.210560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.211650</td>\n",
       "      <td>0.210560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.642718</td>\n",
       "      <td>0.735351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.708738</td>\n",
       "      <td>0.959433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.279612</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.211650</td>\n",
       "      <td>0.210560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.652427</td>\n",
       "      <td>0.737283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.700971</td>\n",
       "      <td>0.922730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.712621</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.285437</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>100.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.650485</td>\n",
       "      <td>0.737927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>100.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.685437</td>\n",
       "      <td>0.905988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>100.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.667961</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>100.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.712621</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.285437</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         C   gamma  test_score  train_score\n",
       "0     0.01    0.01    0.211650     0.210560\n",
       "1     0.01    0.10    0.211650     0.210560\n",
       "2     0.01    1.00    0.211650     0.210560\n",
       "3     0.01   10.00    0.211650     0.210560\n",
       "4     0.01  100.00    0.211650     0.210560\n",
       "5     0.10    0.01    0.211650     0.210560\n",
       "6     0.10    0.10    0.211650     0.210560\n",
       "7     0.10    1.00    0.211650     0.210560\n",
       "8     0.10   10.00    0.322330     0.364456\n",
       "9     0.10  100.00    0.211650     0.210560\n",
       "10    1.00    0.01    0.211650     0.210560\n",
       "11    1.00    0.10    0.211650     0.210560\n",
       "12    1.00    1.00    0.642718     0.735351\n",
       "13    1.00   10.00    0.708738     0.959433\n",
       "14    1.00  100.00    0.279612     1.000000\n",
       "15   10.00    0.01    0.211650     0.210560\n",
       "16   10.00    0.10    0.652427     0.737283\n",
       "17   10.00    1.00    0.700971     0.922730\n",
       "18   10.00   10.00    0.712621     1.000000\n",
       "19   10.00  100.00    0.285437     1.000000\n",
       "20  100.00    0.01    0.650485     0.737927\n",
       "21  100.00    0.10    0.685437     0.905988\n",
       "22  100.00    1.00    0.667961     1.000000\n",
       "23  100.00   10.00    0.712621     1.000000\n",
       "24  100.00  100.00    0.285437     1.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame({'C': _C, 'gamma': _gamma, 'train_score': _train, 'test_score': _test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_clf = svm.SVC(C=10, gamma=10, probability=True)\n",
    "svm_clf.fit(train_x, train_y)\n",
    "train_score_SVM = svm_clf.score(train_x, train_y)\n",
    "test_score_SVM = svm_clf.score(test_x, test_y)\n",
    "probs_SVM = svm_clf.predict_proba(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfNB = MultinomialNB()\n",
    "clfNB.fit(train_xBin, train_yBin)\n",
    "train_score_NB_bin = clfNB.score(train_xBin, train_yBin)\n",
    "test_score_NB_bin = clfNB.score(test_xBin,test_yBin)\n",
    "probs_NB_bin = clfNB.predict_proba(test_xBin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfLR = LogisticRegression()\n",
    "clfLR.fit(train_x_reg,train_y_reg)\n",
    "train_score_LR = clfLR.score(train_x_reg,train_y_reg)\n",
    "test_score_LR = clfLR.score(test_x_reg,test_y_reg)\n",
    "probs_LR = clfLR.predict_proba(test_x_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tempRF = RandomForestClassifier(n_estimators=2000, max_depth=250)\n",
    "tempRF.fit(train_xBin, train_yBin)\n",
    "train_score_RF = tempRF.score(train_xBin, train_yBin)\n",
    "test_score_RF = tempRF.score(test_xBin, test_yBin)\n",
    "probs_RF = tempRF.predict_proba(test_xBin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[84,  1, 10,  8,  6],\n",
       "       [ 5, 70,  9,  5,  9],\n",
       "       [ 7, 12, 80,  3,  6],\n",
       "       [16,  1,  7, 65,  9],\n",
       "       [ 4,  7,  4,  2, 85]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y, svm_clf.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[90,  1,  6, 10,  2],\n",
       "       [ 3, 70, 11,  6,  8],\n",
       "       [10, 10, 73,  6,  9],\n",
       "       [19,  1,  4, 71,  3],\n",
       "       [ 2,  4,  1,  2, 93]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_yBin,clfNB.predict(test_xBin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[85,  1,  7,  9,  7],\n",
       "       [ 2, 73, 14,  5,  4],\n",
       "       [ 7, 16, 74,  5,  6],\n",
       "       [14,  1,  7, 74,  2],\n",
       "       [ 4,  6,  4,  5, 83]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y_reg, clfLR.predict(test_x_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[74,  1, 16,  7, 11],\n",
       "       [ 5, 68, 10,  6,  9],\n",
       "       [ 4,  8, 74,  7, 15],\n",
       "       [17,  3, 12, 58,  8],\n",
       "       [ 3,  4,  3,  2, 90]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_yBin,tempRF.predict(test_xBin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74563106796116507"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score_SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77087378640776694"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score_NB_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75533980582524274"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70679611650485441"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble_probs = probs_SVM + probs_LR + probs_NB_bin + probs_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble_predicted = ensemble_probs.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78640776699029125"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ensemble_predicted == test_y) / float(len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[88,  0, 10,  7,  4],\n",
       "       [ 3, 76, 11,  3,  5],\n",
       "       [ 7, 11, 77,  5,  8],\n",
       "       [17,  1,  4, 72,  4],\n",
       "       [ 3,  3,  3,  1, 92]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y, ensemble_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.99300979e-04,   1.99720391e-03,   9.98601957e-05, ...,\n",
       "          2.59636509e-03,   1.99720391e-04,   2.99580587e-04],\n",
       "       [  6.35889610e-04,   3.81533766e-04,   5.08711688e-04, ...,\n",
       "          3.81533766e-04,   1.27177922e-04,   1.27177922e-04],\n",
       "       [  5.98157674e-04,   2.63189377e-03,   3.58894605e-04, ...,\n",
       "          4.78526139e-04,   1.19631535e-04,   3.58894605e-04],\n",
       "       [  4.26030461e-04,   1.38459900e-03,   1.06507615e-04, ...,\n",
       "          2.23665992e-03,   7.45553307e-04,   5.32538076e-04],\n",
       "       [  2.64865581e-04,   1.72162627e-03,   5.29731161e-04, ...,\n",
       "          7.94596742e-04,   2.64865581e-04,   2.64865581e-04]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_probs = np.exp(clfNB.feature_log_prob_)\n",
    "\n",
    "np.size(feature_probs)\n",
    "test = np.reshape(feature_probs, (5,numWords))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1000)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(feature_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00724914,  0.00737632,  0.0075035 ,  0.00763068,  0.00788503,\n",
       "        0.01042859,  0.00915681,  0.00826656,  0.00966552,  0.01068295])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = np.argpartition(test[1,:],-10)[-10:]\n",
    "best_words = np.zeros((5,10))\n",
    "best_probs = np.zeros((5,10))\n",
    "test[1,ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    ind = np.argpartition(test[i,:],-10)[-10:]\n",
    "    best_words[i,:] = ind\n",
    "    best_probs[i,:] = feature_probs[i,ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_mat = np.zeros((5,numWords))\n",
    "for i in range(0,numWords):\n",
    "    for j in range(0,5):\n",
    "        new_mat[j,i] = feature_probs[j,i]/np.sum(feature_probs[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'president'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(vocab)[best_words[0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_words2 = np.zeros((5,50))\n",
    "best_probs2 = np.zeros((5,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    ind = np.argpartition(new_mat[i,:],-50)[-50:]\n",
    "    best_probs2[i,:] = new_mat[i,ind]\n",
    "    best_words2[i,:] = ind[np.argsort(best_probs2[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_words2 = np.array(best_words2, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sort = np.argsort(best_probs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.48692551,  0.49165605,  0.49672604,  0.49802756,  0.49983493,\n",
       "        0.5010798 ,  0.50410489,  0.50705204,  0.507431  ,  0.50776566,\n",
       "        0.50918804,  0.5096656 ,  0.50987117,  0.51323796,  0.51476922,\n",
       "        0.52090624,  0.52128589,  0.52926985,  0.53367306,  0.53617697,\n",
       "        0.53918911,  0.53977914,  0.54072398,  0.54836952,  0.55239963,\n",
       "        0.55536931,  0.55716319,  0.56023796,  0.56238546,  0.56602366,\n",
       "        0.56756767,  0.57593938,  0.60944718,  0.61553915,  0.61721699,\n",
       "        0.61770576,  0.62466382,  0.62628515,  0.62665614,  0.64952272,\n",
       "        0.65331049,  0.65421051,  0.66730946,  0.69391551,  0.69530092,\n",
       "        0.7060179 ,  0.71746336,  0.73879546,  0.77024708,  0.84166363])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestwords2[sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'steagall', u'loophole', u'tomorrow', u'institutions', u'public',\n",
       "       u'veterans', u'youth', u'virtually', u'poverty', u'super', u'glass',\n",
       "       u'dictator', u'billionaire', u'african', u'huge', u'income',\n",
       "       u'terms', u'opposition', u'involved', u'international', u'medicare',\n",
       "       u'jail', u'corporations', u'street', u'pharmaceutical', u'hour',\n",
       "       u'rigged', u'marijuana', u'committee', u'lower', u'earth',\n",
       "       u'guarantee', u'unemployment', u'contributions', u'wealth',\n",
       "       u'corrupt', u'view', u'class', u'colleges', u'large', u'revolution',\n",
       "       u'billionaires', u'decent', u'universities', u'medical', u'major',\n",
       "       u'vermont', u'secretary', u'finance', u'handful'], \n",
       "      dtype='<U14')"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(vocab)[best_words2[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'proposed', u'kind', u'tried', u'arabs', u'met', u'forward',\n",
       "       u'universal', u'racism', u'drumpf', u'toward', u'tough', u'hear',\n",
       "       u'fund', u'move', u'difficult', u'hard', u'sure', u'comprehensive',\n",
       "       u'investment', u'appropriate', u'prevent', u'enforcement',\n",
       "       u'prescription', u'specific', u'chance', u'figure', u'kinds',\n",
       "       u'try', u'improve', u'ahead', u'brothers', u'results', u'malley',\n",
       "       u'certainly', u'fighters', u'equal', u'others', u'message',\n",
       "       u'opportunity', u'russians', u'agenda', u'questions', u'possible',\n",
       "       u'ways', u'plans', u'incomes', u'affordable', u'senator',\n",
       "       u'particularly', u'sanders'], \n",
       "      dtype='<U14')"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(vocab)[best_words2[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'liberal', u'payroll', u'ethanol', u'winning', u'corruption',\n",
       "       u'principles', u'defending', u'judgment', u'focused', u'harry',\n",
       "       u'amendment', u'secure', u'arms', u'dad', u'words', u'maria',\n",
       "       u'ought', u'focus', u'marco', u'playing', u'marriage', u'everyone',\n",
       "       u'texas', u'supreme', u'carter', u'indeed', u'reid',\n",
       "       u'overwhelming', u'donald', u'subsidies', u'washington', u'simple',\n",
       "       u'nominate', u'commander', u'liberty', u'chief', u'whatsoever',\n",
       "       u'rubio', u'court', u'irs', u'org', u'voters', u'growth', u'fed',\n",
       "       u'islamic', u'flat', u'note', u'schumer', u'amnesty', u'religious'], \n",
       "      dtype='<U14')"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(vocab)[best_words2[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'attack', u'less', u'tonight', u'control', u'elect', u'dream',\n",
       "       u'cannot', u'jihadists', u'regulatory', u'starting', u'already',\n",
       "       u'cheering', u'greatest', u'trust', u'choose', u'society', u'ever',\n",
       "       u'changed', u'pro', u'force', u'threat', u'parents', u'immigration',\n",
       "       u'human', u'service', u'especially', u'code', u'serious',\n",
       "       u'regulations', u'repeal', u'space', u'future', u'hire', u'fully',\n",
       "       u'growing', u'criminals', u'simply', u'sunni', u'someone', u'shia',\n",
       "       u'paycheck', u'prove', u'jihadist', u'replace', u'access',\n",
       "       u'allowed', u'st', u'operating', u'century', u'enterprise'], \n",
       "      dtype='<U14')"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(vocab)[best_words2[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'ted', u'company', u'stronger', u'china', u'thousands', u'great',\n",
       "       u'self', u'read', u'trillions', u'poll', u'everybody', u'polls',\n",
       "       u'economically', u'pouring', u'oh', u'smart', u'concerned', u'love',\n",
       "       u'biggest', u'domain', u'horrible', u'trillion', u'roads', u'vets',\n",
       "       u'thinking', u'advantage', u'anywhere', u'oil', u'losing', u'along',\n",
       "       u'worst', u'heads', u'trade', u'watched', u'probably', u'sitting',\n",
       "       u'built', u'politicians', u'japan', u'ok', u'mexico', u'mess',\n",
       "       u'anymore', u'total', u'nice', u'jeb', u'frankly', u'nobody',\n",
       "       u'totally', u'tremendous'], \n",
       "      dtype='<U14')"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(vocab)[best_words2[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.99803362,  0.64781966,  0.99895486,  0.96523869,  0.99708544,\n",
       "        0.99999875,  0.97733364,  0.99523622,  0.99777496,  0.71699963,\n",
       "        0.75322482,  0.80756365,  0.46526746,  0.8803524 ,  0.40627449,\n",
       "        0.84776015,  0.85707391,  0.95802608])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(probs_NB_bin,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.36456548e-04,   1.20830300e-05,   1.43171563e-05,\n",
       "          9.98033621e-01,   1.10352200e-03],\n",
       "       [  2.89692639e-01,   1.60740976e-02,   2.38422714e-02,\n",
       "          6.47819661e-01,   2.25713306e-02],\n",
       "       [  1.38129251e-05,   1.63104690e-07,   1.95438979e-07,\n",
       "          9.98954859e-01,   1.03096973e-03],\n",
       "       [  6.75705105e-03,   9.65238695e-01,   2.76383480e-02,\n",
       "          2.57010640e-05,   3.40205153e-04],\n",
       "       [  2.54323148e-06,   9.97085442e-01,   7.30673839e-05,\n",
       "          6.95808828e-04,   2.14313832e-03],\n",
       "       [  3.58376671e-09,   9.99998753e-01,   1.09766134e-06,\n",
       "          1.26521048e-07,   1.90870045e-08],\n",
       "       [  9.77333635e-01,   3.73490012e-04,   9.62204572e-03,\n",
       "          5.76210496e-04,   1.20946187e-02],\n",
       "       [  9.95236220e-01,   5.67282367e-05,   2.42386191e-04,\n",
       "          4.16278564e-03,   3.01879942e-04],\n",
       "       [  9.97774957e-01,   1.76616775e-06,   2.26308871e-04,\n",
       "          1.99674318e-03,   2.24481822e-07],\n",
       "       [  7.16999633e-01,   3.17705886e-02,   1.72958665e-02,\n",
       "          3.22240978e-03,   2.30711502e-01],\n",
       "       [  1.55935218e-04,   2.46448928e-01,   7.53224823e-01,\n",
       "          1.05205023e-04,   6.51091380e-05],\n",
       "       [  5.88884872e-03,   1.73778628e-01,   8.07563654e-01,\n",
       "          1.26412952e-02,   1.27574011e-04],\n",
       "       [  3.81083718e-02,   1.48474106e-02,   4.18123702e-01,\n",
       "          6.36530579e-02,   4.65267457e-01],\n",
       "       [  4.35552448e-02,   7.58449437e-02,   8.80352402e-01,\n",
       "          2.41528384e-04,   5.88129349e-06],\n",
       "       [  3.61321048e-01,   2.29908445e-04,   8.65265190e-03,\n",
       "          2.23521901e-01,   4.06274491e-01],\n",
       "       [  5.94814088e-04,   6.67199149e-02,   8.47760146e-01,\n",
       "          6.90140566e-04,   8.42349841e-02],\n",
       "       [  3.20023715e-03,   2.01649701e-03,   3.31614190e-02,\n",
       "          1.04547940e-01,   8.57073907e-01],\n",
       "       [  2.07046087e-03,   1.86665932e-04,   3.83348253e-02,\n",
       "          1.38197231e-03,   9.58026076e-01]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_NB_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.99803362,  0.64781966,  0.99895486,  0.96523869,  0.99708544,\n",
       "        0.99999875,  0.97733364,  0.99523622,  0.99777496,  0.71699963,\n",
       "        0.75322482,  0.80756365,  0.46526746,  0.8803524 ,  0.40627449,\n",
       "        0.84776015,  0.85707391,  0.95802608])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(probs_NB_bin,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
